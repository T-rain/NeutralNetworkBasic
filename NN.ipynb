{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要講NN當然要先講Machine Learning\n",
    "一句話 for Machine Learning  \n",
    "Field of study that gives computers the ability to learn without being explicitly programmed.  \n",
    "-Arthur Lee Samuel, 1959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework\n",
    "1. Define a set of functions\n",
    "2. Evaluate and Search\n",
    "3. Pick the best function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review for Chapter 1 : Tensorflow Flow\n",
    "1. Import or generate datasets\n",
    "2. Transform and normalize data\n",
    "3. Partition datasets into train, test, and validation sets\n",
    "4. Set algorithm parameters  \n",
    "   examples:  \n",
    "   learning_rate = 0.01  \n",
    "   batch_size = 100  \n",
    "   iterations = 1000  \n",
    "5. Initialize variables and placeholders  \n",
    "   variables:用來被更新的參數  \n",
    "   placeholders:佔個位子的參數，之後會放資料進去\n",
    "6. Define the model structure\n",
    "7. Declare the loss functions\n",
    "8. Initialize and train the model\n",
    "9. Evaluate the model\n",
    "10. Tune hyperparameters  \n",
    "    Most of the time, we will want to go back and change some of the hyperparamters, based on     the model performance\n",
    "11. Deploy/predict new outcomes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review for Chapter 1 : Create tensors in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fixed tensors\n",
    "    * zero filled tensor : tf.zeros([row_dim, col_dim])\n",
    "    * one filled tensor : tf.ones([row_dim, col_dim])\n",
    "    * Create a tensor out of an existing constant : tf.constant([1,2,3])\n",
    "* Sequence tensors\n",
    "    * Sequence tensor [6, 9, 12] : tf.range(start=6, limit=15, delta=3)\n",
    "* Random tensors\n",
    "    * Random numbers from a normal distribution: tf.random_normal([row_dim, col_dim],\n",
    "   mean=0.0, stddev=1.0)\n",
    "    * Random numbers from a uniform distribution: tf.random_uniform([row_dim, col_dim],minval=0, maxval=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Deep Learning\n",
    "由許多層的 neurons 互相連結而形成 neural network\n",
    "![](nn_picture/nn-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 為什麼叫做 Deep Learning\n",
    "當 hidden layers 層數夠多 (一般而言大於三層)，就稱為 Deep neural network  \n",
    "像是 CNN,RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](nn_picture/nn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](nn_picture/nn-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing Operational Gates\n",
    "\n",
    "---\n",
    "\n",
    "PracticeGates.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以可以看到我們要做的事情是: 找出所有參數中的 loss 最小值 \n",
    "\n",
    "$\\theta^*=arg min_{\\theta}L(\\theta)$\n",
    "\n",
    " \n",
    "這是一個最佳化問題  \n",
    "可以用 Gradient Descent 更快的找到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent\n",
    "一種 heuristic 最佳化方法(探索性的發現結果),適用於連續、可微的目標函數  \n",
    "核心精神:每一步都朝著進步的方向,直到沒辦法再進步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient descent:\n",
    "* Gradient 受 loss function 影響\n",
    "* Gradient 受 activation function 影響\n",
    "* 受 learning rate 影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "Three of the most commonly used activation functions are \n",
    " * sigmoid (sig)\n",
    " * hyperbolic tangent (tanh)\n",
    " * ramp function (ReLu)\n",
    "\n",
    "plotted in figure 7.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing a one-layer Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "SingleHiddenLayer.ipynb\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross-entropy\n",
    "Cross-entropy is a way to measure distances between probabilities.   \n",
    "Here we want to measure the difference between certainty (0 or 1) and our model probability(0<x<1).  \n",
    "TensorFlow implements cross entropy with the sigmoid function built in.  \n",
    "Also, it is important as part of the hyperparameter tuning to find the best loss function, learning rate, and optimization algorithm for the problem at hand.   \n",
    "For brevity in this recipe, we do not include hyperparameter tuning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-5.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving_linear_regression\n",
    "\n",
    "---\n",
    "\n",
    "ImproveLinear.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "1. [TENSORFLOW_MACHINE_LEARNING_COOKBOOK](https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook)\n",
    "2. [Machine_Learning_with_TensorFlow](https://www.manning.com/books/machine-learning-with-tensorflow)\n",
    "1. [手把手的深度學習實務](https://www.slideshare.net/tw_dsconf/ss-70083878)\n",
    "2. [Hung-yi Lee](http://speech.ee.ntu.edu.tw/~tlkagk/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of neural network that tries to learn parameters that make the output as close to the input as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But an autoencoder is more interesting than that. It contains a small hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding is a great way to reduce the dimension of the input.  \n",
    "For example, if we can represent a 256 by 256 image in just 100 hidden nodes, then we’ve reduced each data item by a factor of hundreds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所謂hyper-parameters，就是機器學習模型裡面的框架參數，它們跟訓練過程中學習的參數（權重）是不一樣的，通常是手工設定，不斷試錯調整，像是learning rate，而最近也開始有人研究基於梯度的超參數學習方法：DrMAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-9-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_picture/nn-9-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "\n",
    "---\n",
    "\n",
    "Autoencoder.ipynb\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern autoencoders\n",
    "* ### stacked autoencoder  \n",
    "A stacked autoencoder starts the same way a normal autoencoder does.  \n",
    "    It learns the encoding for an input into a smaller hidden layer by minimizing the reconstruction error.\n",
    "\n",
    "* ### denoising autoencoder  \n",
    "A denoising autoencoder receives a noised up input instead of the original input, and it tries to “denoise” it.  \n",
    "    The cost function is no longer to minimize the reconstruction error.   \n",
    "    Instead,try to minimize the error between the denoised image and the original image. \n",
    "\n",
    "* ### variational autoencoder  \n",
    "A variational autoencoder can generate new natural images given the hidden variables directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "1. [TENSORFLOW_MACHINE_LEARNING_COOKBOOK](https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook)\n",
    "2. [Machine_Learning_with_TensorFlow](https://www.manning.com/books/machine-learning-with-tensorflow)\n",
    "2. [Hung-yi Lee](http://speech.ee.ntu.edu.tw/~tlkagk/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
